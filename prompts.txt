On Windows, give me the Python code to do that. When launching the code, it must detect which files in which folders have potential changes with metadata comparison, refine the comparison with rclone dry-run if it can be done with little data transfer, and update the remote data while optimizing transfers.
Example of optimization : There is only A and B are cloud storages for type 1 folders, F is a new heavy folder to upload, and C a folder with little changes to update. A stores all of C. A has enough bandwidth left for the day to upload F or update C but not both. B has enough bandwidth left for the day to upload F. The program must choose to upload F in B and update C. If it chose to upload F in A, then it would have to wait for the next day to update C, or to transfer all C in B again, even the parts that are already in A.
If possible : If there is a new heavy file to transfer, but there is only little spaces available in several cloud storages, it must be stored by pieces. 